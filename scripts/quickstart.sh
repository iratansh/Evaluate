#!/bin/bash

# AI Interviewer - Quick Start Script
# This script sets up everything needed for development

set -e  # Exit on any error

echo "ğŸš€ AI Interviewer - Quick Start"
echo "==============================="

# Check if we're in the right directory
if [ ! -f "docker-compose.yml" ]; then
    echo "âŒ Please run this script from the ai-interviewer project root directory"
    exit 1
fi

echo "ğŸ“ Current directory: $(pwd)"

# Step 1: Initialize project
echo ""
echo "ğŸ“¦ Step 1: Initializing project..."
python3 scripts/init.py

# Step 2: Install dependencies (for IDE support)
echo ""
echo "ğŸ“¦ Step 2: Installing frontend dependencies..."
cd frontend && npm install && cd ..

echo ""
echo "ğŸ“¦ Building backend..."
cd backend && ./mvnw dependency:resolve -B && cd ..

# Step 3: Start development environment
echo ""
echo "ğŸš€ Step 3: Starting development environment..."
echo "This will:"
echo "  - Start backend API server (http://localhost:8000)"
echo "  - Start frontend dev server (http://localhost:3000)" 
echo "  - Start Ollama LLM server (http://localhost:11434)"
echo "  - Download Llama 3.2 model (~4GB, may take a while on first run)"
echo ""

read -p "Continue? (y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    make dev
    
    echo ""
    echo "ğŸ‰ Development environment started!"
    echo ""
    echo "ğŸ“ Access points:"
    echo "  â€¢ Frontend:    http://localhost:3000"
    echo "  â€¢ Backend API: http://localhost:8000"
    echo "  â€¢ API Docs:    http://localhost:8000/docs"
    echo "  â€¢ Ollama:      http://localhost:11434"
    echo ""
    echo "ğŸ”§ Useful commands:"
    echo "  â€¢ make logs    - View container logs"
    echo "  â€¢ make clean   - Stop and clean up"
    echo "  â€¢ make help    - Show all commands"
    echo ""
    echo "ğŸ’¡ Tip: The first run will download the Ollama model."
    echo "    Check logs with 'make logs' if needed."
else
    echo "Setup cancelled. You can start development later with 'make dev'"
fi
